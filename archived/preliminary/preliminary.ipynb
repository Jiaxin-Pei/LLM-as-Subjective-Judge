{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "19e2d6cf126c4f79a8f86af8d46809a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"google/flan-t5-xl\", device_map=\"auto\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google/flan-t5-xl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "T5ForConditionalGeneration(\n",
       "  (shared): Embedding(32128, 2048)\n",
       "  (encoder): T5Stack(\n",
       "    (embed_tokens): Embedding(32128, 2048)\n",
       "    (block): ModuleList(\n",
       "      (0): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (relative_attention_bias): Embedding(32, 32)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedActDense(\n",
       "              (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "              (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "              (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1-23): 23 x T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedActDense(\n",
       "              (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "              (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "              (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_layer_norm): T5LayerNorm()\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (decoder): T5Stack(\n",
       "    (embed_tokens): Embedding(32128, 2048)\n",
       "    (block): ModuleList(\n",
       "      (0): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (relative_attention_bias): Embedding(32, 32)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedActDense(\n",
       "              (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "              (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "              (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1-23): 23 x T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedActDense(\n",
       "              (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "              (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "              (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_layer_norm): T5LayerNorm()\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=2048, out_features=32128, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GdDataset(Dataset):\n",
    "    def __init__(self, tokenizer, prefix, dataframe, max_len=512):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.prefix = prefix\n",
    "        self.dataframe = dataframe\n",
    "        self.max_len =  max_len\n",
    "\n",
    "        self.inputs = []\n",
    "        self._build()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.inputs)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        input_ids = self.inputs[index][\"input_ids\"].squeeze()\n",
    "        attention_mask = self.inputs[index][\"attention_mask\"].squeeze()\n",
    "\n",
    "        return {\"input_ids\":input_ids, \"attention_mask\":attention_mask}\n",
    "\n",
    "    def _build(self):\n",
    "        for row in self.dataframe.itertuples(index=False):\n",
    "            tokenized_inputs = self.tokenizer.batch_encode_plus(\n",
    "                [self.prefix + row.text], max_length=self.max_len, pad_to_max_length=True, return_tensors=\"pt\"\n",
    "                )\n",
    "            self.inputs.append(tokenized_inputs)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"agg.csv\")\n",
    "dataframe = data[['instance_id', 'text', 'man_score', 'woman_score',\n",
    "                  'white_score', 'black_score', 'asian_score', 'nonwhite_score',\n",
    "                  'offensiveness_label']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"politeness_1500.csv\")\n",
    "dataframe = data[['instance_id', 'text', 'man_score', 'woman_score',\n",
    "                  'white_score', 'black_score', 'asian_score', 'nonwhite_score',\n",
    "                  'label']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe = pd.read_csv(\"offensiveness_results.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2398: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "prefix = \"How do you rate how offensive the sentence is from 1 (not offensive at all) to 5 (very offensive). Options: 1, 2, 3, 4, 5: \"\n",
    "dataset = GdDataset(tokenizer, prefix, dataframe)\n",
    "loader = DataLoader(dataset, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/47 [00:00<?, ?it/s]/opt/anaconda/lib/python3.9/site-packages/transformers/generation/utils.py:1355: UserWarning: Using `max_length`'s default (20) to control the generation length. This behaviour is deprecated and will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "100%|██████████| 47/47 [02:33<00:00,  3.27s/it]\n"
     ]
    }
   ],
   "source": [
    "outputs = []\n",
    "\n",
    "for batch in tqdm(loader):\n",
    "    outs = model.generate(input_ids=batch[\"input_ids\"].cuda(),\n",
    "                          attention_mask=batch[\"attention_mask\"].cuda())\n",
    "    dec = [tokenizer.decode(ids) for ids in outs]\n",
    "    outputs.extend(dec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def func_1(x):\n",
    "    try:\n",
    "        return float(x)\n",
    "    except ValueError:\n",
    "        return np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe['t5_base'] = [func_1(re.findall(r'<pad> (.*)</s>|$', i)[0]) for i in outputs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe.to_csv(\"offensiveness_results.csv\", index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Offensiveness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe.to_csv(\"offensiveness_results.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>offensiveness_label</th>\n",
       "      <th>woman_score</th>\n",
       "      <th>man_score</th>\n",
       "      <th>t5_female</th>\n",
       "      <th>t5_male</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>offensiveness_label</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.886194</td>\n",
       "      <td>0.859597</td>\n",
       "      <td>0.572129</td>\n",
       "      <td>0.559087</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>woman_score</th>\n",
       "      <td>0.886194</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.588440</td>\n",
       "      <td>0.508472</td>\n",
       "      <td>0.505532</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>man_score</th>\n",
       "      <td>0.859597</td>\n",
       "      <td>0.588440</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.480209</td>\n",
       "      <td>0.457415</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>t5_female</th>\n",
       "      <td>0.572129</td>\n",
       "      <td>0.508472</td>\n",
       "      <td>0.480209</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.944830</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>t5_male</th>\n",
       "      <td>0.559087</td>\n",
       "      <td>0.505532</td>\n",
       "      <td>0.457415</td>\n",
       "      <td>0.944830</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     offensiveness_label  woman_score  man_score  t5_female   \n",
       "offensiveness_label             1.000000     0.886194   0.859597   0.572129  \\\n",
       "woman_score                     0.886194     1.000000   0.588440   0.508472   \n",
       "man_score                       0.859597     0.588440   1.000000   0.480209   \n",
       "t5_female                       0.572129     0.508472   0.480209   1.000000   \n",
       "t5_male                         0.559087     0.505532   0.457415   0.944830   \n",
       "\n",
       "                      t5_male  \n",
       "offensiveness_label  0.559087  \n",
       "woman_score          0.505532  \n",
       "man_score            0.457415  \n",
       "t5_female            0.944830  \n",
       "t5_male              1.000000  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataframe[['offensiveness_label', 'woman_score', 'man_score', 't5_female', 't5_male']].corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>offensiveness_label</th>\n",
       "      <th>white_score</th>\n",
       "      <th>black_score</th>\n",
       "      <th>asian_score</th>\n",
       "      <th>nonwhite_score</th>\n",
       "      <th>t5_white</th>\n",
       "      <th>t5_black</th>\n",
       "      <th>t5_asian</th>\n",
       "      <th>t5_nwhite</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>offensiveness_label</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.951949</td>\n",
       "      <td>0.665420</td>\n",
       "      <td>0.607914</td>\n",
       "      <td>0.702225</td>\n",
       "      <td>0.532454</td>\n",
       "      <td>0.431274</td>\n",
       "      <td>0.477106</td>\n",
       "      <td>0.487406</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>white_score</th>\n",
       "      <td>0.951949</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.459541</td>\n",
       "      <td>0.412191</td>\n",
       "      <td>0.485856</td>\n",
       "      <td>0.510847</td>\n",
       "      <td>0.413723</td>\n",
       "      <td>0.453439</td>\n",
       "      <td>0.462110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>black_score</th>\n",
       "      <td>0.665420</td>\n",
       "      <td>0.459541</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.313313</td>\n",
       "      <td>0.883433</td>\n",
       "      <td>0.336824</td>\n",
       "      <td>0.294883</td>\n",
       "      <td>0.306738</td>\n",
       "      <td>0.306076</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>asian_score</th>\n",
       "      <td>0.607914</td>\n",
       "      <td>0.412191</td>\n",
       "      <td>0.313313</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.833277</td>\n",
       "      <td>0.281539</td>\n",
       "      <td>0.198625</td>\n",
       "      <td>0.255149</td>\n",
       "      <td>0.280688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nonwhite_score</th>\n",
       "      <td>0.702225</td>\n",
       "      <td>0.485856</td>\n",
       "      <td>0.883433</td>\n",
       "      <td>0.833277</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.368690</td>\n",
       "      <td>0.288952</td>\n",
       "      <td>0.342738</td>\n",
       "      <td>0.349146</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>t5_white</th>\n",
       "      <td>0.532454</td>\n",
       "      <td>0.510847</td>\n",
       "      <td>0.336824</td>\n",
       "      <td>0.281539</td>\n",
       "      <td>0.368690</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.680823</td>\n",
       "      <td>0.775075</td>\n",
       "      <td>0.844475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>t5_black</th>\n",
       "      <td>0.431274</td>\n",
       "      <td>0.413723</td>\n",
       "      <td>0.294883</td>\n",
       "      <td>0.198625</td>\n",
       "      <td>0.288952</td>\n",
       "      <td>0.680823</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.702795</td>\n",
       "      <td>0.681454</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>t5_asian</th>\n",
       "      <td>0.477106</td>\n",
       "      <td>0.453439</td>\n",
       "      <td>0.306738</td>\n",
       "      <td>0.255149</td>\n",
       "      <td>0.342738</td>\n",
       "      <td>0.775075</td>\n",
       "      <td>0.702795</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.752964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>t5_nwhite</th>\n",
       "      <td>0.487406</td>\n",
       "      <td>0.462110</td>\n",
       "      <td>0.306076</td>\n",
       "      <td>0.280688</td>\n",
       "      <td>0.349146</td>\n",
       "      <td>0.844475</td>\n",
       "      <td>0.681454</td>\n",
       "      <td>0.752964</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     offensiveness_label  white_score  black_score   \n",
       "offensiveness_label             1.000000     0.951949     0.665420  \\\n",
       "white_score                     0.951949     1.000000     0.459541   \n",
       "black_score                     0.665420     0.459541     1.000000   \n",
       "asian_score                     0.607914     0.412191     0.313313   \n",
       "nonwhite_score                  0.702225     0.485856     0.883433   \n",
       "t5_white                        0.532454     0.510847     0.336824   \n",
       "t5_black                        0.431274     0.413723     0.294883   \n",
       "t5_asian                        0.477106     0.453439     0.306738   \n",
       "t5_nwhite                       0.487406     0.462110     0.306076   \n",
       "\n",
       "                     asian_score  nonwhite_score  t5_white  t5_black   \n",
       "offensiveness_label     0.607914        0.702225  0.532454  0.431274  \\\n",
       "white_score             0.412191        0.485856  0.510847  0.413723   \n",
       "black_score             0.313313        0.883433  0.336824  0.294883   \n",
       "asian_score             1.000000        0.833277  0.281539  0.198625   \n",
       "nonwhite_score          0.833277        1.000000  0.368690  0.288952   \n",
       "t5_white                0.281539        0.368690  1.000000  0.680823   \n",
       "t5_black                0.198625        0.288952  0.680823  1.000000   \n",
       "t5_asian                0.255149        0.342738  0.775075  0.702795   \n",
       "t5_nwhite               0.280688        0.349146  0.844475  0.681454   \n",
       "\n",
       "                     t5_asian  t5_nwhite  \n",
       "offensiveness_label  0.477106   0.487406  \n",
       "white_score          0.453439   0.462110  \n",
       "black_score          0.306738   0.306076  \n",
       "asian_score          0.255149   0.280688  \n",
       "nonwhite_score       0.342738   0.349146  \n",
       "t5_white             0.775075   0.844475  \n",
       "t5_black             0.702795   0.681454  \n",
       "t5_asian             1.000000   0.752964  \n",
       "t5_nwhite            0.752964   1.000000  "
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataframe[['offensiveness_label', 'white_score', 'black_score', 'asian_score', 'nonwhite_score', 't5_white', 't5_black', 't5_asian', 't5_nwhite']].corr()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Politeness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe.to_csv(\"politeness_results.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>woman_score</th>\n",
       "      <th>man_score</th>\n",
       "      <th>t5_female</th>\n",
       "      <th>t5_male</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>label</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.913951</td>\n",
       "      <td>0.871574</td>\n",
       "      <td>0.704577</td>\n",
       "      <td>0.696097</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>woman_score</th>\n",
       "      <td>0.913951</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.661776</td>\n",
       "      <td>0.642404</td>\n",
       "      <td>0.629376</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>man_score</th>\n",
       "      <td>0.871574</td>\n",
       "      <td>0.661776</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.621085</td>\n",
       "      <td>0.612807</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>t5_female</th>\n",
       "      <td>0.704577</td>\n",
       "      <td>0.642404</td>\n",
       "      <td>0.621085</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.914906</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>t5_male</th>\n",
       "      <td>0.696097</td>\n",
       "      <td>0.629376</td>\n",
       "      <td>0.612807</td>\n",
       "      <td>0.914906</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                label  woman_score  man_score  t5_female   t5_male\n",
       "label        1.000000     0.913951   0.871574   0.704577  0.696097\n",
       "woman_score  0.913951     1.000000   0.661776   0.642404  0.629376\n",
       "man_score    0.871574     0.661776   1.000000   0.621085  0.612807\n",
       "t5_female    0.704577     0.642404   0.621085   1.000000  0.914906\n",
       "t5_male      0.696097     0.629376   0.612807   0.914906  1.000000"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataframe[['label', 'woman_score', 'man_score', 't5_female', 't5_male']].corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>white_score</th>\n",
       "      <th>black_score</th>\n",
       "      <th>asian_score</th>\n",
       "      <th>nonwhite_score</th>\n",
       "      <th>t5_white</th>\n",
       "      <th>t5_black</th>\n",
       "      <th>t5_asian</th>\n",
       "      <th>t5_nwhite</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>label</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.961691</td>\n",
       "      <td>0.734787</td>\n",
       "      <td>0.740195</td>\n",
       "      <td>0.799760</td>\n",
       "      <td>0.717389</td>\n",
       "      <td>0.699355</td>\n",
       "      <td>0.664146</td>\n",
       "      <td>0.670903</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>white_score</th>\n",
       "      <td>0.961691</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.570726</td>\n",
       "      <td>0.607198</td>\n",
       "      <td>0.628572</td>\n",
       "      <td>0.699500</td>\n",
       "      <td>0.684615</td>\n",
       "      <td>0.651077</td>\n",
       "      <td>0.657932</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>black_score</th>\n",
       "      <td>0.734787</td>\n",
       "      <td>0.570726</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.453552</td>\n",
       "      <td>0.897044</td>\n",
       "      <td>0.513196</td>\n",
       "      <td>0.503193</td>\n",
       "      <td>0.466550</td>\n",
       "      <td>0.482531</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>asian_score</th>\n",
       "      <td>0.740195</td>\n",
       "      <td>0.607198</td>\n",
       "      <td>0.453552</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.854298</td>\n",
       "      <td>0.520972</td>\n",
       "      <td>0.510125</td>\n",
       "      <td>0.498755</td>\n",
       "      <td>0.490712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nonwhite_score</th>\n",
       "      <td>0.799760</td>\n",
       "      <td>0.628572</td>\n",
       "      <td>0.897044</td>\n",
       "      <td>0.854298</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.549166</td>\n",
       "      <td>0.533041</td>\n",
       "      <td>0.503124</td>\n",
       "      <td>0.506105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>t5_white</th>\n",
       "      <td>0.717389</td>\n",
       "      <td>0.699500</td>\n",
       "      <td>0.513196</td>\n",
       "      <td>0.520972</td>\n",
       "      <td>0.549166</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.883490</td>\n",
       "      <td>0.789990</td>\n",
       "      <td>0.821255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>t5_black</th>\n",
       "      <td>0.699355</td>\n",
       "      <td>0.684615</td>\n",
       "      <td>0.503193</td>\n",
       "      <td>0.510125</td>\n",
       "      <td>0.533041</td>\n",
       "      <td>0.883490</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.847226</td>\n",
       "      <td>0.891406</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>t5_asian</th>\n",
       "      <td>0.664146</td>\n",
       "      <td>0.651077</td>\n",
       "      <td>0.466550</td>\n",
       "      <td>0.498755</td>\n",
       "      <td>0.503124</td>\n",
       "      <td>0.789990</td>\n",
       "      <td>0.847226</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.870262</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>t5_nwhite</th>\n",
       "      <td>0.670903</td>\n",
       "      <td>0.657932</td>\n",
       "      <td>0.482531</td>\n",
       "      <td>0.490712</td>\n",
       "      <td>0.506105</td>\n",
       "      <td>0.821255</td>\n",
       "      <td>0.891406</td>\n",
       "      <td>0.870262</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   label  white_score  black_score  asian_score   \n",
       "label           1.000000     0.961691     0.734787     0.740195  \\\n",
       "white_score     0.961691     1.000000     0.570726     0.607198   \n",
       "black_score     0.734787     0.570726     1.000000     0.453552   \n",
       "asian_score     0.740195     0.607198     0.453552     1.000000   \n",
       "nonwhite_score  0.799760     0.628572     0.897044     0.854298   \n",
       "t5_white        0.717389     0.699500     0.513196     0.520972   \n",
       "t5_black        0.699355     0.684615     0.503193     0.510125   \n",
       "t5_asian        0.664146     0.651077     0.466550     0.498755   \n",
       "t5_nwhite       0.670903     0.657932     0.482531     0.490712   \n",
       "\n",
       "                nonwhite_score  t5_white  t5_black  t5_asian  t5_nwhite  \n",
       "label                 0.799760  0.717389  0.699355  0.664146   0.670903  \n",
       "white_score           0.628572  0.699500  0.684615  0.651077   0.657932  \n",
       "black_score           0.897044  0.513196  0.503193  0.466550   0.482531  \n",
       "asian_score           0.854298  0.520972  0.510125  0.498755   0.490712  \n",
       "nonwhite_score        1.000000  0.549166  0.533041  0.503124   0.506105  \n",
       "t5_white              0.549166  1.000000  0.883490  0.789990   0.821255  \n",
       "t5_black              0.533041  0.883490  1.000000  0.847226   0.891406  \n",
       "t5_asian              0.503124  0.789990  0.847226  1.000000   0.870262  \n",
       "t5_nwhite             0.506105  0.821255  0.891406  0.870262   1.000000  "
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataframe[['label', 'white_score', 'black_score', 'asian_score', 'nonwhite_score', 't5_white', 't5_black', 't5_asian', 't5_nwhite']].corr()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Random Nouns Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "nouns = pd.read_csv(\"noun_list.csv\", header=0)\n",
    "noun_list = [i for i in nouns.loc[0:20, 'word']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/21 [00:00<?, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/opt/anaconda/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2364: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "/opt/anaconda/lib/python3.9/site-packages/transformers/generation/utils.py:1346: UserWarning: Using `max_length`'s default (20) to control the generation length. This behaviour is deprecated and will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "/opt/anaconda/lib/python3.9/site-packages/transformers/generation/utils.py:1346: UserWarning: Using `max_length`'s default (20) to control the generation length. This behaviour is deprecated and will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "100%|██████████| 47/47 [02:35<00:00,  3.31s/it]\n",
      "  5%|▍         | 1/21 [02:36<52:01, 156.10s/it]/opt/anaconda/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2364: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "100%|██████████| 47/47 [02:37<00:00,  3.34s/it]\n",
      "100%|██████████| 47/47 [02:37<00:00,  3.35s/it]\n",
      "100%|██████████| 47/47 [02:37<00:00,  3.35s/it]\n",
      "100%|██████████| 47/47 [02:37<00:00,  3.35s/it]\n",
      "100%|██████████| 47/47 [02:37<00:00,  3.35s/it]\n",
      "100%|██████████| 47/47 [02:37<00:00,  3.35s/it]\n",
      "100%|██████████| 47/47 [02:37<00:00,  3.35s/it]\n",
      "100%|██████████| 47/47 [02:37<00:00,  3.35s/it]\n",
      "100%|██████████| 47/47 [02:37<00:00,  3.35s/it]\n",
      "100%|██████████| 47/47 [02:37<00:00,  3.35s/it]]\n",
      "100%|██████████| 47/47 [02:37<00:00,  3.36s/it]]\n",
      "100%|██████████| 47/47 [02:37<00:00,  3.35s/it]]\n",
      "100%|██████████| 47/47 [02:37<00:00,  3.35s/it]]\n",
      "100%|██████████| 47/47 [02:37<00:00,  3.35s/it]]\n",
      "100%|██████████| 47/47 [02:37<00:00,  3.35s/it]]\n",
      "100%|██████████| 47/47 [02:37<00:00,  3.35s/it]]\n",
      "100%|██████████| 47/47 [02:37<00:00,  3.35s/it]]\n",
      "100%|██████████| 47/47 [02:37<00:00,  3.35s/it]]\n",
      "100%|██████████| 47/47 [02:37<00:00,  3.35s/it]]\n",
      "100%|██████████| 47/47 [02:37<00:00,  3.35s/it]]\n",
      "100%|██████████| 21/21 [55:13<00:00, 157.79s/it]\n"
     ]
    }
   ],
   "source": [
    "results = {}\n",
    "\n",
    "for i in tqdm(noun_list):\n",
    "    prefix = f\"How does a {i} rate how offensive the sentence is from 1 (not offensive at all) to 5 (very offensive). Options: 1, 2, 3, 4, 5: \"\n",
    "    dataset = GdDataset(tokenizer, prefix, dataframe)\n",
    "    loader = DataLoader(dataset, batch_size=32)\n",
    "\n",
    "    outputs = []\n",
    "    for batch in tqdm(loader):\n",
    "        outs = model.generate(input_ids=batch[\"input_ids\"].cuda(),\n",
    "                              attention_mask=batch[\"attention_mask\"].cuda())\n",
    "        dec = [tokenizer.decode(ids) for ids in outs]\n",
    "        outputs.extend(dec)\n",
    "    \n",
    "    results[i] = [func_1(re.findall(r'<pad> (.*)</s>|$', i)[0]) for i in outputs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame.from_dict(results).to_csv(\"offensiveness_base_random_results.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"politeness.csv\")\n",
    "dataframe = data[['instance_id', 'text', 'man_score', 'woman_score',\n",
    "                  'white_score', 'black_score', 'asian_score', 'nonwhite_score',\n",
    "                  'label']].sample(n=1500, replace=False)\n",
    "dataframe.to_csv(\"politeness_1500.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/21 [00:00<?, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/opt/anaconda/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2398: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "/opt/anaconda/lib/python3.9/site-packages/transformers/generation/utils.py:1355: UserWarning: Using `max_length`'s default (20) to control the generation length. This behaviour is deprecated and will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "100%|██████████| 21/21 [54:41<00:00, 156.28s/it]\n",
      "100%|██████████| 21/21 [54:30<00:00, 155.76s/it]\n"
     ]
    }
   ],
   "source": [
    "for j in [\"male \", \"female \"]:\n",
    "    results = {}\n",
    "\n",
    "    for i in tqdm(noun_list):\n",
    "        prefix = f\"How does a {j}{i} rate how polite the sentence is from 1 (not polite at all) to 5 (very polite). Options: 1, 2, 3, 4, 5: \"\n",
    "        dataset = GdDataset(tokenizer, prefix, dataframe)\n",
    "        loader = DataLoader(dataset, batch_size=32)\n",
    "\n",
    "        outputs = []\n",
    "        for batch in loader:\n",
    "            outs = model.generate(input_ids=batch[\"input_ids\"].cuda(),\n",
    "                                attention_mask=batch[\"attention_mask\"].cuda())\n",
    "            dec = [tokenizer.decode(ids) for ids in outs]\n",
    "            outputs.extend(dec)\n",
    "        \n",
    "        results[i] = [func_1(re.findall(r'<pad> (.*)</s>|$', i)[0]) for i in outputs]\n",
    "\n",
    "    pd.DataFrame.from_dict(results).to_csv(f\"politeness_{j}_random_results.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-19-0871b5857c55>:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dataframe['t5_scores'] = [re.findall(r'<pad> (.*)</s>|$', i)[0] for i in outputs]\n",
      "<ipython-input-19-0871b5857c55>:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dataframe['t5_scores'] = dataframe['t5_scores'].apply(func_1)\n"
     ]
    }
   ],
   "source": [
    "dataframe['t5_scores'] = [re.findall(r'<pad> (.*)</s>|$', i)[0] for i in outputs]\n",
    "dataframe['t5_scores'] = dataframe['t5_scores'].apply(func_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe[['instance_id', 'offensiveness_label', 'woman_score', 'man_score', 't5_scores']].to_csv(\"flant5_xl_offensiveness_test.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>offensiveness_label</th>\n",
       "      <th>white_score</th>\n",
       "      <th>black_score</th>\n",
       "      <th>asian_score</th>\n",
       "      <th>woman_score</th>\n",
       "      <th>man_score</th>\n",
       "      <th>t5_scores</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>offensiveness_label</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.951949</td>\n",
       "      <td>0.665420</td>\n",
       "      <td>0.607914</td>\n",
       "      <td>0.886194</td>\n",
       "      <td>0.859597</td>\n",
       "      <td>0.559087</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>white_score</th>\n",
       "      <td>0.951949</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.459541</td>\n",
       "      <td>0.412191</td>\n",
       "      <td>0.834325</td>\n",
       "      <td>0.819814</td>\n",
       "      <td>0.546923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>black_score</th>\n",
       "      <td>0.665420</td>\n",
       "      <td>0.459541</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.313313</td>\n",
       "      <td>0.619013</td>\n",
       "      <td>0.546541</td>\n",
       "      <td>0.312050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>asian_score</th>\n",
       "      <td>0.607914</td>\n",
       "      <td>0.412191</td>\n",
       "      <td>0.313313</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.541024</td>\n",
       "      <td>0.536235</td>\n",
       "      <td>0.335882</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>woman_score</th>\n",
       "      <td>0.886194</td>\n",
       "      <td>0.834325</td>\n",
       "      <td>0.619013</td>\n",
       "      <td>0.541024</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.588440</td>\n",
       "      <td>0.505532</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>man_score</th>\n",
       "      <td>0.859597</td>\n",
       "      <td>0.819814</td>\n",
       "      <td>0.546541</td>\n",
       "      <td>0.536235</td>\n",
       "      <td>0.588440</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.457415</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>t5_scores</th>\n",
       "      <td>0.559087</td>\n",
       "      <td>0.546923</td>\n",
       "      <td>0.312050</td>\n",
       "      <td>0.335882</td>\n",
       "      <td>0.505532</td>\n",
       "      <td>0.457415</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     offensiveness_label  white_score  black_score   \n",
       "offensiveness_label             1.000000     0.951949     0.665420  \\\n",
       "white_score                     0.951949     1.000000     0.459541   \n",
       "black_score                     0.665420     0.459541     1.000000   \n",
       "asian_score                     0.607914     0.412191     0.313313   \n",
       "woman_score                     0.886194     0.834325     0.619013   \n",
       "man_score                       0.859597     0.819814     0.546541   \n",
       "t5_scores                       0.559087     0.546923     0.312050   \n",
       "\n",
       "                     asian_score  woman_score  man_score  t5_scores  \n",
       "offensiveness_label     0.607914     0.886194   0.859597   0.559087  \n",
       "white_score             0.412191     0.834325   0.819814   0.546923  \n",
       "black_score             0.313313     0.619013   0.546541   0.312050  \n",
       "asian_score             1.000000     0.541024   0.536235   0.335882  \n",
       "woman_score             0.541024     1.000000   0.588440   0.505532  \n",
       "man_score               0.536235     0.588440   1.000000   0.457415  \n",
       "t5_scores               0.335882     0.505532   0.457415   1.000000  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataframe[['offensiveness_label', 'white_score', 'black_score', 'asian_score', 'woman_score', 'man_score', 't5_scores']].corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.618"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataframe[\"t5_scores\"].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "del model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def func_1(x):\n",
    "    try:\n",
    "        return float(x)\n",
    "    except ValueError:\n",
    "        return np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-9-56d6734a7c11>:14: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dataframe_1['t5_all'] = data_all['t5_scores'].apply(func_1)\n",
      "<ipython-input-9-56d6734a7c11>:15: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dataframe_1['t5_person'] = data_person['t5_scores'].apply(func_1)\n",
      "<ipython-input-9-56d6734a7c11>:16: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dataframe_1['t5_white'] = data_white['t5_scores'].apply(func_1)\n",
      "<ipython-input-9-56d6734a7c11>:17: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dataframe_1['t5_black'] = data_black['t5_scores'].apply(func_1)\n",
      "<ipython-input-9-56d6734a7c11>:18: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dataframe_1['t5_asian'] = data_asian['t5_scores'].apply(func_1)\n",
      "<ipython-input-9-56d6734a7c11>:19: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dataframe_1['t5_nwhite'] = data_nwhite['t5_scores'].apply(func_1)\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv(\"agg.csv\")\n",
    "dataframe = data[['instance_id', 'offensiveness_gold', 'text', 'scores', 'users', 'man_score',\n",
    "                  'woman_score', 'white_score', 'black_score', 'asian_score', 'nonwhite_score',\n",
    "                  'gender', 'scores_num', 'scores_std', 'offensiveness_label']]\n",
    "data_all = pd.read_csv(\"flant5_xl_offensiveness.csv\")\n",
    "data_person = pd.read_csv(\"flant5_xl_offensiveness_p.csv\")\n",
    "data_white = pd.read_csv(\"flant5_xl_offensiveness_white.csv\")\n",
    "data_black = pd.read_csv(\"flant5_xl_offensiveness_black.csv\")\n",
    "data_asian = pd.read_csv(\"flant5_xl_offensiveness_asian.csv\")\n",
    "data_nwhite = pd.read_csv(\"flant5_xl_offensiveness_nwhite.csv\")\n",
    "\n",
    "dataframe_1 = dataframe[['instance_id', 'text', 'offensiveness_label', 'white_score', 'black_score', 'asian_score', 'nonwhite_score']]\n",
    "\n",
    "dataframe_1['t5_all'] = data_all['t5_scores'].apply(func_1)\n",
    "dataframe_1['t5_person'] = data_person['t5_scores'].apply(func_1)\n",
    "dataframe_1['t5_white'] = data_white['t5_scores'].apply(func_1)\n",
    "dataframe_1['t5_black'] = data_black['t5_scores'].apply(func_1)\n",
    "dataframe_1['t5_asian'] = data_asian['t5_scores'].apply(func_1)\n",
    "dataframe_1['t5_nwhite'] = data_nwhite['t5_scores'].apply(func_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe_1.rename(columns={\"offensiveness_label\":\"all_score\"}).to_csv(\"flant5_xl_offensiveness_ethnicity.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>offensiveness_label</th>\n",
       "      <th>white_score</th>\n",
       "      <th>black_score</th>\n",
       "      <th>asian_score</th>\n",
       "      <th>nonwhite_score</th>\n",
       "      <th>t5_all</th>\n",
       "      <th>t5_person</th>\n",
       "      <th>t5_white</th>\n",
       "      <th>t5_black</th>\n",
       "      <th>t5_asian</th>\n",
       "      <th>t5_nwhite</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>offensiveness_label</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.951949</td>\n",
       "      <td>0.665420</td>\n",
       "      <td>0.607914</td>\n",
       "      <td>0.702225</td>\n",
       "      <td>0.419916</td>\n",
       "      <td>0.307335</td>\n",
       "      <td>0.176328</td>\n",
       "      <td>0.163355</td>\n",
       "      <td>0.149649</td>\n",
       "      <td>0.155313</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>white_score</th>\n",
       "      <td>0.951949</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.459541</td>\n",
       "      <td>0.412191</td>\n",
       "      <td>0.485856</td>\n",
       "      <td>0.391969</td>\n",
       "      <td>0.285608</td>\n",
       "      <td>0.168220</td>\n",
       "      <td>0.159977</td>\n",
       "      <td>0.143175</td>\n",
       "      <td>0.150031</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>black_score</th>\n",
       "      <td>0.665420</td>\n",
       "      <td>0.459541</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.313313</td>\n",
       "      <td>0.883433</td>\n",
       "      <td>0.266330</td>\n",
       "      <td>0.230419</td>\n",
       "      <td>0.131767</td>\n",
       "      <td>0.139887</td>\n",
       "      <td>0.110495</td>\n",
       "      <td>0.099941</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>asian_score</th>\n",
       "      <td>0.607914</td>\n",
       "      <td>0.412191</td>\n",
       "      <td>0.313313</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.833277</td>\n",
       "      <td>0.279010</td>\n",
       "      <td>0.181890</td>\n",
       "      <td>0.131149</td>\n",
       "      <td>0.050126</td>\n",
       "      <td>0.051511</td>\n",
       "      <td>0.119543</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nonwhite_score</th>\n",
       "      <td>0.702225</td>\n",
       "      <td>0.485856</td>\n",
       "      <td>0.883433</td>\n",
       "      <td>0.833277</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.309465</td>\n",
       "      <td>0.245027</td>\n",
       "      <td>0.146192</td>\n",
       "      <td>0.108549</td>\n",
       "      <td>0.109905</td>\n",
       "      <td>0.110638</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>t5_all</th>\n",
       "      <td>0.419916</td>\n",
       "      <td>0.391969</td>\n",
       "      <td>0.266330</td>\n",
       "      <td>0.279010</td>\n",
       "      <td>0.309465</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.778491</td>\n",
       "      <td>0.506982</td>\n",
       "      <td>0.350188</td>\n",
       "      <td>0.445223</td>\n",
       "      <td>0.426360</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>t5_person</th>\n",
       "      <td>0.307335</td>\n",
       "      <td>0.285608</td>\n",
       "      <td>0.230419</td>\n",
       "      <td>0.181890</td>\n",
       "      <td>0.245027</td>\n",
       "      <td>0.778491</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.527179</td>\n",
       "      <td>0.328732</td>\n",
       "      <td>0.470252</td>\n",
       "      <td>0.429291</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>t5_white</th>\n",
       "      <td>0.176328</td>\n",
       "      <td>0.168220</td>\n",
       "      <td>0.131767</td>\n",
       "      <td>0.131149</td>\n",
       "      <td>0.146192</td>\n",
       "      <td>0.506982</td>\n",
       "      <td>0.527179</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.567880</td>\n",
       "      <td>0.690999</td>\n",
       "      <td>0.774324</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>t5_black</th>\n",
       "      <td>0.163355</td>\n",
       "      <td>0.159977</td>\n",
       "      <td>0.139887</td>\n",
       "      <td>0.050126</td>\n",
       "      <td>0.108549</td>\n",
       "      <td>0.350188</td>\n",
       "      <td>0.328732</td>\n",
       "      <td>0.567880</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.578954</td>\n",
       "      <td>0.654904</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>t5_asian</th>\n",
       "      <td>0.149649</td>\n",
       "      <td>0.143175</td>\n",
       "      <td>0.110495</td>\n",
       "      <td>0.051511</td>\n",
       "      <td>0.109905</td>\n",
       "      <td>0.445223</td>\n",
       "      <td>0.470252</td>\n",
       "      <td>0.690999</td>\n",
       "      <td>0.578954</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.686525</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>t5_nwhite</th>\n",
       "      <td>0.155313</td>\n",
       "      <td>0.150031</td>\n",
       "      <td>0.099941</td>\n",
       "      <td>0.119543</td>\n",
       "      <td>0.110638</td>\n",
       "      <td>0.426360</td>\n",
       "      <td>0.429291</td>\n",
       "      <td>0.774324</td>\n",
       "      <td>0.654904</td>\n",
       "      <td>0.686525</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     offensiveness_label  white_score  black_score   \n",
       "offensiveness_label             1.000000     0.951949     0.665420  \\\n",
       "white_score                     0.951949     1.000000     0.459541   \n",
       "black_score                     0.665420     0.459541     1.000000   \n",
       "asian_score                     0.607914     0.412191     0.313313   \n",
       "nonwhite_score                  0.702225     0.485856     0.883433   \n",
       "t5_all                          0.419916     0.391969     0.266330   \n",
       "t5_person                       0.307335     0.285608     0.230419   \n",
       "t5_white                        0.176328     0.168220     0.131767   \n",
       "t5_black                        0.163355     0.159977     0.139887   \n",
       "t5_asian                        0.149649     0.143175     0.110495   \n",
       "t5_nwhite                       0.155313     0.150031     0.099941   \n",
       "\n",
       "                     asian_score  nonwhite_score    t5_all  t5_person   \n",
       "offensiveness_label     0.607914        0.702225  0.419916   0.307335  \\\n",
       "white_score             0.412191        0.485856  0.391969   0.285608   \n",
       "black_score             0.313313        0.883433  0.266330   0.230419   \n",
       "asian_score             1.000000        0.833277  0.279010   0.181890   \n",
       "nonwhite_score          0.833277        1.000000  0.309465   0.245027   \n",
       "t5_all                  0.279010        0.309465  1.000000   0.778491   \n",
       "t5_person               0.181890        0.245027  0.778491   1.000000   \n",
       "t5_white                0.131149        0.146192  0.506982   0.527179   \n",
       "t5_black                0.050126        0.108549  0.350188   0.328732   \n",
       "t5_asian                0.051511        0.109905  0.445223   0.470252   \n",
       "t5_nwhite               0.119543        0.110638  0.426360   0.429291   \n",
       "\n",
       "                     t5_white  t5_black  t5_asian  t5_nwhite  \n",
       "offensiveness_label  0.176328  0.163355  0.149649   0.155313  \n",
       "white_score          0.168220  0.159977  0.143175   0.150031  \n",
       "black_score          0.131767  0.139887  0.110495   0.099941  \n",
       "asian_score          0.131149  0.050126  0.051511   0.119543  \n",
       "nonwhite_score       0.146192  0.108549  0.109905   0.110638  \n",
       "t5_all               0.506982  0.350188  0.445223   0.426360  \n",
       "t5_person            0.527179  0.328732  0.470252   0.429291  \n",
       "t5_white             1.000000  0.567880  0.690999   0.774324  \n",
       "t5_black             0.567880  1.000000  0.578954   0.654904  \n",
       "t5_asian             0.690999  0.578954  1.000000   0.686525  \n",
       "t5_nwhite            0.774324  0.654904  0.686525   1.000000  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataframe_1.drop(['instance_id', 'text'], axis=1).corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>all_score</th>\n",
       "      <th>woman_score</th>\n",
       "      <th>man_score</th>\n",
       "      <th>t5_all</th>\n",
       "      <th>t5_person</th>\n",
       "      <th>t5_female</th>\n",
       "      <th>t5_male</th>\n",
       "      <th>t5_woman</th>\n",
       "      <th>t5_man</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>all_score</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.886194</td>\n",
       "      <td>0.859597</td>\n",
       "      <td>0.419916</td>\n",
       "      <td>0.307335</td>\n",
       "      <td>0.288212</td>\n",
       "      <td>0.372686</td>\n",
       "      <td>0.241227</td>\n",
       "      <td>0.330948</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>woman_score</th>\n",
       "      <td>0.886194</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.588440</td>\n",
       "      <td>0.371136</td>\n",
       "      <td>0.279721</td>\n",
       "      <td>0.244024</td>\n",
       "      <td>0.330173</td>\n",
       "      <td>0.202124</td>\n",
       "      <td>0.291693</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>man_score</th>\n",
       "      <td>0.859597</td>\n",
       "      <td>0.588440</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.374351</td>\n",
       "      <td>0.268690</td>\n",
       "      <td>0.259497</td>\n",
       "      <td>0.326503</td>\n",
       "      <td>0.210024</td>\n",
       "      <td>0.288249</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>t5_all</th>\n",
       "      <td>0.419916</td>\n",
       "      <td>0.371136</td>\n",
       "      <td>0.374351</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.778491</td>\n",
       "      <td>0.677945</td>\n",
       "      <td>0.781278</td>\n",
       "      <td>0.624696</td>\n",
       "      <td>0.742642</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>t5_person</th>\n",
       "      <td>0.307335</td>\n",
       "      <td>0.279721</td>\n",
       "      <td>0.268690</td>\n",
       "      <td>0.778491</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.676446</td>\n",
       "      <td>0.769406</td>\n",
       "      <td>0.630969</td>\n",
       "      <td>0.748259</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>t5_female</th>\n",
       "      <td>0.288212</td>\n",
       "      <td>0.244024</td>\n",
       "      <td>0.259497</td>\n",
       "      <td>0.677945</td>\n",
       "      <td>0.676446</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.794348</td>\n",
       "      <td>0.893926</td>\n",
       "      <td>0.826811</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>t5_male</th>\n",
       "      <td>0.372686</td>\n",
       "      <td>0.330173</td>\n",
       "      <td>0.326503</td>\n",
       "      <td>0.781278</td>\n",
       "      <td>0.769406</td>\n",
       "      <td>0.794348</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.748793</td>\n",
       "      <td>0.888172</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>t5_woman</th>\n",
       "      <td>0.241227</td>\n",
       "      <td>0.202124</td>\n",
       "      <td>0.210024</td>\n",
       "      <td>0.624696</td>\n",
       "      <td>0.630969</td>\n",
       "      <td>0.893926</td>\n",
       "      <td>0.748793</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.784709</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>t5_man</th>\n",
       "      <td>0.330948</td>\n",
       "      <td>0.291693</td>\n",
       "      <td>0.288249</td>\n",
       "      <td>0.742642</td>\n",
       "      <td>0.748259</td>\n",
       "      <td>0.826811</td>\n",
       "      <td>0.888172</td>\n",
       "      <td>0.784709</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             all_score  woman_score  man_score    t5_all  t5_person   \n",
       "all_score     1.000000     0.886194   0.859597  0.419916   0.307335  \\\n",
       "woman_score   0.886194     1.000000   0.588440  0.371136   0.279721   \n",
       "man_score     0.859597     0.588440   1.000000  0.374351   0.268690   \n",
       "t5_all        0.419916     0.371136   0.374351  1.000000   0.778491   \n",
       "t5_person     0.307335     0.279721   0.268690  0.778491   1.000000   \n",
       "t5_female     0.288212     0.244024   0.259497  0.677945   0.676446   \n",
       "t5_male       0.372686     0.330173   0.326503  0.781278   0.769406   \n",
       "t5_woman      0.241227     0.202124   0.210024  0.624696   0.630969   \n",
       "t5_man        0.330948     0.291693   0.288249  0.742642   0.748259   \n",
       "\n",
       "             t5_female   t5_male  t5_woman    t5_man  \n",
       "all_score     0.288212  0.372686  0.241227  0.330948  \n",
       "woman_score   0.244024  0.330173  0.202124  0.291693  \n",
       "man_score     0.259497  0.326503  0.210024  0.288249  \n",
       "t5_all        0.677945  0.781278  0.624696  0.742642  \n",
       "t5_person     0.676446  0.769406  0.630969  0.748259  \n",
       "t5_female     1.000000  0.794348  0.893926  0.826811  \n",
       "t5_male       0.794348  1.000000  0.748793  0.888172  \n",
       "t5_woman      0.893926  0.748793  1.000000  0.784709  \n",
       "t5_man        0.826811  0.888172  0.784709  1.000000  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_csv(\"flant5_xl_offensiveness_gender.csv\").drop(['instance_id', 'text'], axis=1).corr()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random noun test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.stats as stats\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "white = pd.read_csv(\"white_random_results.csv\")\n",
    "black = pd.read_csv('black_random_results.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "white_person = pd.read_csv(\"flant5_xl_offensiveness_ethnicity.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pencil       1.308000\n",
       "phone        1.124667\n",
       "computer     1.195333\n",
       "chair        1.324000\n",
       "table        1.295333\n",
       "carpet       1.182000\n",
       "bookshelf    1.252000\n",
       "lamp         1.172000\n",
       "clock        1.270667\n",
       "window       1.146000\n",
       "cat          1.225333\n",
       "dog          1.080000\n",
       "horse        1.254667\n",
       "bird         1.129333\n",
       "fish         1.098667\n",
       "snake        1.248000\n",
       "rabbit       1.236000\n",
       "elephant     1.181333\n",
       "squirrel     1.194667\n",
       "fox          1.212667\n",
       "school       1.136667\n",
       "dtype: float64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "white.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pencil       1.357333\n",
       "phone        1.081333\n",
       "computer     1.081333\n",
       "chair        1.228000\n",
       "table        1.197333\n",
       "carpet       1.222000\n",
       "bookshelf    1.272000\n",
       "lamp         1.180000\n",
       "clock        1.236000\n",
       "window       1.118667\n",
       "cat          1.204000\n",
       "dog          1.174667\n",
       "horse        1.386667\n",
       "bird         1.112000\n",
       "fish         1.094667\n",
       "snake        1.204000\n",
       "rabbit       1.176000\n",
       "elephant     1.208000\n",
       "squirrel     1.134667\n",
       "fox          1.178667\n",
       "school       1.109333\n",
       "dtype: float64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "black.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.3086666666666666"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "white_person['t5_white'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pencil</th>\n",
       "      <th>phone</th>\n",
       "      <th>computer</th>\n",
       "      <th>chair</th>\n",
       "      <th>table</th>\n",
       "      <th>carpet</th>\n",
       "      <th>bookshelf</th>\n",
       "      <th>lamp</th>\n",
       "      <th>clock</th>\n",
       "      <th>window</th>\n",
       "      <th>...</th>\n",
       "      <th>dog</th>\n",
       "      <th>horse</th>\n",
       "      <th>bird</th>\n",
       "      <th>fish</th>\n",
       "      <th>snake</th>\n",
       "      <th>rabbit</th>\n",
       "      <th>elephant</th>\n",
       "      <th>squirrel</th>\n",
       "      <th>fox</th>\n",
       "      <th>school</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1495</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1496</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1497</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1498</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1499</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1500 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      pencil  phone  computer  chair  table  carpet  bookshelf  lamp  clock   \n",
       "0        1.0    1.0       1.0    1.0    1.0     1.0        1.0   1.0    1.0  \\\n",
       "1        1.0    1.0       1.0    1.0    1.0     1.0        1.0   1.0    1.0   \n",
       "2        1.0    1.0       1.0    1.0    1.0     1.0        1.0   1.0    1.0   \n",
       "3        1.0    1.0       1.0    1.0    1.0     1.0        1.0   1.0    1.0   \n",
       "4        1.0    1.0       1.0    1.0    1.0     1.0        1.0   1.0    1.0   \n",
       "...      ...    ...       ...    ...    ...     ...        ...   ...    ...   \n",
       "1495     1.0    1.0       1.0    1.0    1.0     1.0        1.0   1.0    1.0   \n",
       "1496     1.0    1.0       1.0    1.0    1.0     1.0        1.0   1.0    1.0   \n",
       "1497     1.0    1.0       1.0    1.0    1.0     1.0        1.0   1.0    1.0   \n",
       "1498     1.0    1.0       1.0    1.0    1.0     1.0        1.0   1.0    1.0   \n",
       "1499     1.0    1.0       3.0    1.0    1.0     1.0        1.0   1.0    1.0   \n",
       "\n",
       "      window  ...  dog  horse  bird  fish  snake  rabbit  elephant  squirrel   \n",
       "0        1.0  ...  1.0    1.0   1.0   1.0    1.0     1.0       1.0       1.0  \\\n",
       "1        1.0  ...  1.0    1.0   1.0   1.0    1.0     1.0       1.0       1.0   \n",
       "2        1.0  ...  1.0    1.0   1.0   1.0    1.0     1.0       1.0       1.0   \n",
       "3        1.0  ...  1.0    1.0   1.0   1.0    1.0     1.0       1.0       1.0   \n",
       "4        1.0  ...  1.0    1.0   1.0   1.0    1.0     1.0       1.0       1.0   \n",
       "...      ...  ...  ...    ...   ...   ...    ...     ...       ...       ...   \n",
       "1495     1.0  ...  1.0    1.0   1.0   1.0    1.0     1.0       1.0       1.0   \n",
       "1496     1.0  ...  1.0    1.0   1.0   1.0    1.0     1.0       1.0       1.0   \n",
       "1497     1.0  ...  1.0    1.0   1.0   1.0    1.0     1.0       1.0       1.0   \n",
       "1498     1.0  ...  1.0    1.0   1.0   1.0    1.0     1.0       1.0       1.0   \n",
       "1499     1.0  ...  1.0    1.0   1.0   1.0    3.0     1.0       1.0       1.0   \n",
       "\n",
       "      fox  school  \n",
       "0     1.0     1.0  \n",
       "1     1.0     1.0  \n",
       "2     1.0     1.0  \n",
       "3     1.0     1.0  \n",
       "4     1.0     1.0  \n",
       "...   ...     ...  \n",
       "1495  1.0     1.0  \n",
       "1496  1.0     1.0  \n",
       "1497  1.0     1.0  \n",
       "1498  1.0     1.0  \n",
       "1499  1.0     1.0  \n",
       "\n",
       "[1500 rows x 21 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "white"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-7-e18fb4b62e0b>:1: RuntimeWarning: Precision loss occurred in moment calculation due to catastrophic cancellation. This occurs when the data are nearly identical. Results may be unreliable.\n",
      "  stats.ttest_ind(white.loc[0], black.loc[0], equal_var=False)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Ttest_indResult(statistic=-1.0000000000000009, pvalue=0.32925657717170853)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stats.ttest_ind(white.loc[0], black.loc[0], equal_var=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pandas.core.series.Series"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(white.loc[0])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Supplement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t5_all = [func_1(x) for x in dataframe_1['t5_all']]\n",
    "t5_f = [func_1(x) for x in dataframe_1['t5_female']]\n",
    "t5_m = [func_1(x) for x in dataframe_1['t5_male']]\n",
    "\n",
    "score_all = []\n",
    "score_f = []\n",
    "score_m = []\n",
    "\n",
    "for row in dataframe_1.itertuples():\n",
    "    row_s = row.scores[1:-1]\n",
    "    s = [float(x.strip()) for x in row_s.split(',')]\n",
    "    row_g = row.gender[1:-1]\n",
    "    g = [x.strip() for x in row_g.split(',')]\n",
    "    s_f = [s[i] for i in range(len(s)) if g[i]==\"'Woman'\"]\n",
    "    s_m = [s[i] for i in range(len(s)) if g[i]==\"'Man'\"]\n",
    "\n",
    "    score_all.append(Counter(s).most_common()[0][0])\n",
    "    if s_f:\n",
    "        score_f.append(Counter(s_f).most_common()[0][0])\n",
    "    else:\n",
    "        score_f.append(np.nan)\n",
    "    if s_m:\n",
    "        score_m.append(Counter(s_m).most_common()[0][0])\n",
    "    else:\n",
    "        score_m.append(np.nan)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
